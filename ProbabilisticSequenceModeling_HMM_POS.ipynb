{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#***LIBRARIES***"
      ],
      "metadata": {
        "id": "P_AMCpKCtUD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import math"
      ],
      "metadata": {
        "id": "K1gZbP3TtSS5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Task 1: Vocabulary Creation***"
      ],
      "metadata": {
        "id": "JP0SEiSFNY96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a vocabulary builder, where creates a vocabulary file from a training data corpus with the following key features:\n",
        "\n",
        "* **Word Frequency Analysis**: The code scans the input file and counts the frequency of each word in the corpus.\n",
        "* **Threshold Filtering**: Words appearing fewer than min_freq times are excluded from the vocabulary and counted as unknown words.\n",
        "* **Vocabulary Sorting**: The vocabulary is sorted in descending order based on word frequency.\n",
        "* **Unknown Token Handling**: Adding a special <unk> token at the beginning of the vocabulary to represent all low-frequency words.\n",
        "* **Output Format**: Each vocabulary entry is written in the format word\\tindex\\tfrequency, where the index starts from 1 (with <unk> at index 0).\n",
        "\n",
        "The function also provides statistics about the vocabulary size and the number of words classified as unknown."
      ],
      "metadata": {
        "id": "33Lm55bRYv4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(input_file, output_file, min_freq=3):\n",
        "    freq_dict = collections.defaultdict(int)\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                word = parts[1]\n",
        "                freq_dict[word] += 1\n",
        "\n",
        "    unknown_count = sum(count for word, count in freq_dict.items() if count < min_freq)\n",
        "    vocab_list = [(word, count) for word, count in freq_dict.items() if count >= min_freq]\n",
        "    vocab_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"<unk>\\t0\\t{unknown_count}\\n\")\n",
        "\n",
        "\n",
        "        for i, (word, count) in enumerate(vocab_list):\n",
        "            f.write(f\"{word}\\t{i+1}\\t{count}\\n\")\n",
        "\n",
        "    print(f\"Vocabulary saved to '{output_file}'\")\n",
        "    print(f\"Total vocabulary size: {len(vocab_list) + 1}\")\n",
        "    print(f\"Occurrences of '<unk>': {unknown_count}\")\n",
        "\n",
        "input_file = \"train\"\n",
        "output_file = \"vocab.txt\"\n",
        "threshold = 3\n",
        "build_vocabulary(input_file, output_file, threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-YVa-LOFdV",
        "outputId": "118b9b79-e26f-4920-87f8-1638388558bd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary saved to 'vocab.txt'\n",
            "Total vocabulary size: 16920\n",
            "Occurrences of '<unk>': 32537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Task 2: Model Learning***"
      ],
      "metadata": {
        "id": "TKTj0fBbvk0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a function for training a Hidden Markov Model (HMM) for part-of-speech (POS) tagging. The function builds transition and emission probability matrices from a tagged training corpus with these key features:\n",
        "\n",
        "* **Vocabulary Integration**: The code loads a pre-built vocabulary file, supporting out-of-vocabulary words via the <unk> token.\n",
        "* **Count Collection**: The algorithm traverses the training file to collect:\n",
        "  * Transition counts (frequency of state-to-state transitions)\n",
        "  * Emission counts (frequency of state-word pairings)\n",
        "  * State counts (frequency of each state)\n",
        "\n",
        "* **Probability Calculation**: The function computes:\n",
        "  * Transition probabilities: P(current state | previous state)\n",
        "  * Emission probabilities: P(word | state)\n",
        "\n",
        "* **JSON Model Output**: The resulting probabilities are stored in a JSON file with two main components:\n",
        "  * Transition matrix\n",
        "  * Emission matrix\n",
        "\n",
        "\n",
        "* **Statistics Reporting**: The code outputs useful statistics about the trained model, including:\n",
        "\n",
        "  * Number of unique states\n",
        "  * Total transition parameters\n",
        "  * Total emission parameters\n"
      ],
      "metadata": {
        "id": "t4usYlbFaZ_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "def train_hmm_model(train_file, vocab_file, model_output):\n",
        "    vocabulary = []\n",
        "    with open(vocab_file, 'r', encoding='utf-8') as vf:\n",
        "        for line in vf:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 1:\n",
        "                word = parts[0]\n",
        "                vocabulary.append(word)\n",
        "\n",
        "    print(f\"Loaded vocabulary with {len(vocabulary)} words\")\n",
        "\n",
        "    transition_counts = defaultdict(int)\n",
        "    emission_counts = defaultdict(int)\n",
        "    state_counts = defaultdict(int)\n",
        "    observed_states = set()\n",
        "\n",
        "    with open(train_file, 'r', encoding='utf-8') as file:\n",
        "        prev_state = None\n",
        "\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                prev_state = None\n",
        "                continue\n",
        "\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) < 3:\n",
        "                continue\n",
        "\n",
        "            index, word, state = parts\n",
        "\n",
        "            if word not in vocabulary:\n",
        "                word = \"<unk>\"\n",
        "\n",
        "            emission_counts[(state, word)] += 1\n",
        "            state_counts[state] += 1\n",
        "            observed_states.add(state)\n",
        "\n",
        "            if prev_state is not None:\n",
        "                transition_counts[(prev_state, state)] += 1\n",
        "\n",
        "            prev_state = state\n",
        "\n",
        "    states_list = sorted(list(observed_states))\n",
        "\n",
        "    transition_probabilities = {}\n",
        "    for s in states_list:\n",
        "        for s_prime in states_list:\n",
        "            key = f\"({s}, {s_prime})\"\n",
        "\n",
        "            if (s, s_prime) in transition_counts:\n",
        "                probability = transition_counts[(s, s_prime)] / state_counts[s]\n",
        "            else:\n",
        "                probability = 0\n",
        "\n",
        "            transition_probabilities[key] = probability\n",
        "\n",
        "    emission_probabilities = {}\n",
        "    for s in states_list:\n",
        "        for x in vocabulary:\n",
        "            key = f\"({s}, {x})\"\n",
        "\n",
        "            if (s, x) in emission_counts:\n",
        "                probability = emission_counts[(s, x)] / state_counts[s]\n",
        "            else:\n",
        "                probability = 0\n",
        "\n",
        "            emission_probabilities[key] = probability\n",
        "\n",
        "    hmm_model = {\n",
        "        \"transition\": transition_probabilities,\n",
        "        \"emission\": emission_probabilities\n",
        "    }\n",
        "\n",
        "    with open(model_output, 'w', encoding='utf-8') as f:\n",
        "        json.dump(hmm_model, f, indent=4)\n",
        "\n",
        "    expected_transition_params = len(states_list) * len(states_list)\n",
        "    expected_emission_params = len(states_list) * len(vocabulary)\n",
        "\n",
        "    print(f\"HMM model saved to '{model_output}'\")\n",
        "    print(f\"Total unique states: {len(states_list)}\")\n",
        "    print(f\"Total transition parameters: {len(transition_probabilities)}\")\n",
        "    print(f\"Total emission parameters: {len(emission_probabilities)}\")\n",
        "\n",
        "    return hmm_model"
      ],
      "metadata": {
        "id": "mDenIS6Ew5ta"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_hmm_model(\"train\", \"vocab.txt\", \"hmm.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wXWGYtuxKzn",
        "outputId": "d6537f24-f2e8-4f87-a86c-0fa45d9b7f94"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded vocabulary with 16920 words\n",
            "HMM model saved to 'hmm.json'\n",
            "Total unique states: 45\n",
            "Total transition parameters: 2025\n",
            "Total emission parameters: 761400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads and analyzes the Hidden Markov Model, displaying the highest-probability state transitions and word emissions while reporting statistics on non-zero entries to evaluate model characteristics."
      ],
      "metadata": {
        "id": "fMn2UDeLcUqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"hmm.json\", 'r', encoding='utf-8') as f:\n",
        "    hmm_model = json.load(f)\n",
        "\n",
        "transition_probs = hmm_model[\"transition\"]\n",
        "emission_probs = hmm_model[\"emission\"]\n",
        "\n",
        "print(\"\\nFew Transition Probabilities:\")\n",
        "non_zero_transitions = [(k, v) for k, v in transition_probs.items() if v > 0]\n",
        "non_zero_transitions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for key, prob in non_zero_transitions[:10]:\n",
        "    print(f\"  {key}: {prob:.6f}\")\n",
        "\n",
        "print(\"\\nFew Emission Probabilities:\")\n",
        "non_zero_emissions = [(k, v) for k, v in emission_probs.items() if v > 0]\n",
        "non_zero_emissions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for key, prob in non_zero_emissions[:10]:\n",
        "    print(f\"  {key}: {prob:.6f}\")\n",
        "\n",
        "print(f\"\\nTotal transitions: {len(transition_probs)}\")\n",
        "print(f\"Non-zero transitions: {len(non_zero_transitions)}\")\n",
        "print(f\"Total emissions: {len(emission_probs)}\")\n",
        "print(f\"Non-zero emissions: {len(non_zero_emissions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpGtWKj9-zVb",
        "outputId": "a5822680-fc48-4bd3-f2da-8e3876397f15"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Few Transition Probabilities:\n",
            "  (#, CD): 0.992126\n",
            "  ($, CD): 0.992072\n",
            "  (PDT, DT): 0.918919\n",
            "  (MD, VB): 0.799089\n",
            "  (RBS, JJ): 0.747126\n",
            "  (SYM, :): 0.600000\n",
            "  (TO, VB): 0.577699\n",
            "  (UH, ,): 0.517241\n",
            "  (DT, NN): 0.473488\n",
            "  (EX, VBZ): 0.468187\n",
            "\n",
            "Few Emission Probabilities:\n",
            "  (#, #): 1.000000\n",
            "  (WP$, whose): 1.000000\n",
            "  (,, ,): 0.999914\n",
            "  (TO, to): 0.992591\n",
            "  (., .): 0.988623\n",
            "  (``, ``): 0.983928\n",
            "  ('', ''): 0.981577\n",
            "  ($, $): 0.974773\n",
            "  (POS, 's): 0.927933\n",
            "  (RBS, most): 0.878161\n",
            "\n",
            "Total transitions: 2025\n",
            "Non-zero transitions: 1351\n",
            "Total emissions: 761400\n",
            "Non-zero emissions: 23373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Task 3: Greedy Decoding with HMM***"
      ],
      "metadata": {
        "id": "IwV1KGX8zdkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a greedy decoder for POS tagging that uses a Hidden Markov Model. It processes text word-by-word, selecting the most probable tag for each word based on emission probabilities and the previous tag. The algorithm handles out-of-vocabulary words using an \"unknown\" token and evaluates accuracy when gold standard tags are available."
      ],
      "metadata": {
        "id": "sZV4EJW2ctHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile greedy_decode.py\n",
        "import json\n",
        "def load_vocabulary(vocab_path):\n",
        "    vocabulary = set()\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if parts:\n",
        "                word = parts[0]\n",
        "                vocabulary.add(word)\n",
        "    return vocabulary\n",
        "\n",
        "def load_hmm_model(model_path):\n",
        "    with open(model_path, 'r', encoding='utf-8') as file:\n",
        "        hmm = json.load(file)\n",
        "    transition_probs = {}\n",
        "    for key, value in hmm[\"transition\"].items():\n",
        "        s, s_prime = key.strip(\"()\").split(\", \")\n",
        "        transition_probs[(s, s_prime)] = value\n",
        "\n",
        "    emission_probs = {}\n",
        "    for key, value in hmm[\"emission\"].items():\n",
        "        s, x = key.strip(\"()\").split(\", \")\n",
        "        emission_probs[(s, x)] = value\n",
        "\n",
        "    return transition_probs, emission_probs\n",
        "\n",
        "def greedy_decoding(input_file, model_file, vocab_file, output_file, evaluate=False):\n",
        "    transition_probs, emission_probs = load_hmm_model(model_file)\n",
        "    vocabulary = load_vocabulary(vocab_file)\n",
        "\n",
        "    all_states = set()\n",
        "    for (s, _) in transition_probs.keys():\n",
        "        all_states.add(s)\n",
        "    for (_, s_prime) in transition_probs.keys():\n",
        "        all_states.add(s_prime)\n",
        "\n",
        "    total_words = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file_in, open(output_file, 'w', encoding='utf-8') as file_out:\n",
        "        current_sentence = []\n",
        "        gold_tags = []\n",
        "\n",
        "        for line in file_in:\n",
        "            line = line.strip()\n",
        "\n",
        "            if not line:\n",
        "                if current_sentence:\n",
        "                    prev_state = None\n",
        "                    predicted_tags = []\n",
        "\n",
        "                    for word in current_sentence:\n",
        "                        lookup_word = word if word in vocabulary else \"<unk>\"\n",
        "\n",
        "                        best_state = None\n",
        "                        best_prob = -1\n",
        "\n",
        "                        for state in all_states:\n",
        "                            emission_prob = emission_probs.get((state, lookup_word), 0)\n",
        "\n",
        "                            if prev_state is not None:\n",
        "                                transition_prob = transition_probs.get((prev_state, state), 0)\n",
        "                            else:\n",
        "                                transition_prob = 1\n",
        "                            prob = emission_prob * transition_prob\n",
        "                            if prob > best_prob:\n",
        "                                best_prob = prob\n",
        "                                best_state = state\n",
        "\n",
        "                        if best_state is None:\n",
        "                            best_state = \"NN\"\n",
        "\n",
        "                        predicted_tags.append(best_state)\n",
        "                        prev_state = best_state\n",
        "\n",
        "                    for i, (word, tag) in enumerate(zip(current_sentence, predicted_tags)):\n",
        "                        file_out.write(f\"{i+1}\\t{word}\\t{tag}\\n\")\n",
        "\n",
        "                    if evaluate and gold_tags:\n",
        "                        for pred_tag, gold_tag in zip(predicted_tags, gold_tags):\n",
        "                            total_words += 1\n",
        "                            if pred_tag == gold_tag:\n",
        "                                correct_predictions += 1\n",
        "\n",
        "                file_out.write(\"\\n\")\n",
        "                current_sentence = []\n",
        "                gold_tags = []\n",
        "                continue\n",
        "\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                word = parts[1]\n",
        "                current_sentence.append(word)\n",
        "\n",
        "                if evaluate and len(parts) >= 3:\n",
        "                    gold_tags.append(parts[2])\n",
        "\n",
        "    accuracy = correct_predictions / total_words if total_words > 0 else 0\n",
        "    if evaluate:\n",
        "        print(f\"Accuracy on dev data: {accuracy:.4f}\")\n",
        "\n",
        "    return accuracy if evaluate else None\n",
        "\n",
        "\n",
        "vocab_file = \"vocab.txt\"\n",
        "dev_file = \"dev\"\n",
        "test_file = \"test\"\n",
        "model_file = \"hmm.json\"\n",
        "dev_output = \"dev_greedy.out\"\n",
        "test_output = \"greedy.out\"\n",
        "\n",
        "accuracy = greedy_decoding(dev_file, model_file, vocab_file, dev_output, evaluate=True)\n",
        "print(f\"Accuracy on dev data: {accuracy:.4f}\")\n",
        "\n",
        "greedy_decoding(test_file, model_file, vocab_file, test_output)\n",
        "print(f\"Predictions for test data saved to '{test_output}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B08LzjkX0n2w",
        "outputId": "60914ee1-62cd-41cc-a3f6-cd3fc78093dd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing greedy_decode.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python greedy_decode.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULc-3inKOvQZ",
        "outputId": "81fce2d8-a084-400f-8016-400d37895bcd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on dev data: 0.9231\n",
            "Accuracy on dev data: 0.9231\n",
            "Predictions for test data saved to 'greedy.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command is running an evaluation script (eval.py) to compare the performance of greedy decoder against a gold standard. The script is taking two parameters:\n",
        "\n",
        "* -g dev: Specifies the gold standard file (containing the correct POS tags)\n",
        "* -p dev_greedy.out: Specifies prediction file (containing the tags predicted by greedy decoder)"
      ],
      "metadata": {
        "id": "MxaR1ILFc4Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval.py -g dev -p dev_greedy.out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi8HHNmoOoXU",
        "outputId": "965eaa8d-792f-4690-d140-62d45289b300"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'1\\tThat\\tDT' '38\\t.\\t.' 131751\n",
            "total: 131751, correct: 121623, accuracy: 92.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function that displays the first several lines of an output file, useful for quickly inspecting POS tagging results. It counts total tokens and sentences in the file and handles potential errors. The function is applied to view both 'greedy.out' (test results) and 'dev_greedy.out' (development set results)."
      ],
      "metadata": {
        "id": "jxSRmm0_dHxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_output_file(file_path, num_lines=20):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        print(f\"Showing first {min(num_lines, len(lines))} lines of {file_path}:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for i, line in enumerate(lines[:num_lines]):\n",
        "            print(f\"{i+1:>3}: {line.rstrip()}\")\n",
        "\n",
        "        if len(lines) > num_lines:\n",
        "            print(f\"... and {len(lines) - num_lines} more lines\")\n",
        "\n",
        "        sentences = sum(1 for line in lines if line.strip() == '')\n",
        "        tokens = sum(1 for line in lines if line.strip() != '')\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"File contains {tokens} tokens in {sentences} sentences\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "\n",
        "view_output_file('greedy.out', 30)\n",
        "\n",
        "print(\"\\n\")\n",
        "view_output_file('dev_greedy.out', 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndelombTPmaZ",
        "outputId": "705e906c-8027-4a68-c5db-3f73abe9b131"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing first 30 lines of greedy.out:\n",
            "--------------------------------------------------\n",
            "  1: 1\tInfluential\tFW\n",
            "  2: 2\tmembers\tNNS\n",
            "  3: 3\tof\tIN\n",
            "  4: 4\tthe\tDT\n",
            "  5: 5\tHouse\tNNP\n",
            "  6: 6\tWays\tNNPS\n",
            "  7: 7\tand\tCC\n",
            "  8: 8\tMeans\tNNP\n",
            "  9: 9\tCommittee\tNNP\n",
            " 10: 10\tintroduced\tVBD\n",
            " 11: 11\tlegislation\tNN\n",
            " 12: 12\tthat\tIN\n",
            " 13: 13\twould\tMD\n",
            " 14: 14\trestrict\tVB\n",
            " 15: 15\thow\tWRB\n",
            " 16: 16\tthe\tDT\n",
            " 17: 17\tnew\tJJ\n",
            " 18: 18\tsavings-and-loan\tNN\n",
            " 19: 19\tbailout\tNN\n",
            " 20: 20\tagency\tNN\n",
            " 21: 21\tcan\tMD\n",
            " 22: 22\traise\tVB\n",
            " 23: 23\tcapital\tNN\n",
            " 24: 24\t,\t,\n",
            " 25: 25\tcreating\tVBG\n",
            " 26: 26\tanother\tDT\n",
            " 27: 27\tpotential\tJJ\n",
            " 28: 28\tobstacle\tNN\n",
            " 29: 29\tto\tTO\n",
            " 30: 30\tthe\tDT\n",
            "... and 135055 more lines\n",
            "--------------------------------------------------\n",
            "File contains 129624 tokens in 5461 sentences\n",
            "\n",
            "\n",
            "Showing first 30 lines of dev_greedy.out:\n",
            "--------------------------------------------------\n",
            "  1: 1\tThe\tDT\n",
            "  2: 2\tArizona\tNNP\n",
            "  3: 3\tCorporations\tNNS\n",
            "  4: 4\tCommission\tNNP\n",
            "  5: 5\tauthorized\tVBD\n",
            "  6: 6\tan\tDT\n",
            "  7: 7\t11.5\tCD\n",
            "  8: 8\t%\tNN\n",
            "  9: 9\trate\tNN\n",
            " 10: 10\tincrease\tNN\n",
            " 11: 11\tat\tIN\n",
            " 12: 12\tTucson\tNNP\n",
            " 13: 13\tElectric\tNNP\n",
            " 14: 14\tPower\tNNP\n",
            " 15: 15\tCo.\tNNP\n",
            " 16: 16\t,\t,\n",
            " 17: 17\tsubstantially\tRB\n",
            " 18: 18\tlower\tJJR\n",
            " 19: 19\tthan\tIN\n",
            " 20: 20\trecommended\tJJ\n",
            " 21: 21\tlast\tJJ\n",
            " 22: 22\tmonth\tNN\n",
            " 23: 23\tby\tIN\n",
            " 24: 24\ta\tDT\n",
            " 25: 25\tcommission\tNN\n",
            " 26: 26\thearing\tNN\n",
            " 27: 27\tofficer\tNN\n",
            " 28: 28\tand\tCC\n",
            " 29: 29\tbarely\tRB\n",
            " 30: 30\thalf\tPDT\n",
            "... and 137247 more lines\n",
            "--------------------------------------------------\n",
            "File contains 131751 tokens in 5526 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Task 4: Viterbi Decoding with HMM***"
      ],
      "metadata": {
        "id": "PjGd6aMoUqDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math"
      ],
      "metadata": {
        "id": "wsrmHiGMToqz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function loads an HMM model from a JSON file, restructuring the transition and emission probabilities into nested dictionaries for more efficient access. It converts string keys like \"(NNP, CD)\" into a hierarchical structure where transition_probs[s][s_prime] gives the probability of moving from state s to s_prime."
      ],
      "metadata": {
        "id": "dej4MWktdWOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hmm_model(model_path):\n",
        "    with open(model_path, 'r', encoding='utf-8') as file:\n",
        "        hmm = json.load(file)\n",
        "\n",
        "    transition_probs = {}\n",
        "    for key, value in hmm[\"transition\"].items():\n",
        "        s, s_prime = key.strip(\"()\").split(\", \")\n",
        "        if s not in transition_probs:\n",
        "            transition_probs[s] = {}\n",
        "        transition_probs[s][s_prime] = value\n",
        "\n",
        "    emission_probs = {}\n",
        "    for key, value in hmm[\"emission\"].items():\n",
        "        s, x = key.strip(\"()\").split(\", \")\n",
        "        if s not in emission_probs:\n",
        "            emission_probs[s] = {}\n",
        "        emission_probs[s][x] = value\n",
        "\n",
        "    return transition_probs, emission_probs"
      ],
      "metadata": {
        "id": "TnbZMFH1RTSC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function implements Viterbi decoding for POS tagging. It processes input text sentence by sentence, applies the Viterbi algorithm to find the optimal tag sequence, writes predictions to an output file, and calculates accuracy when in evaluation mode by comparing against gold standard tags."
      ],
      "metadata": {
        "id": "GI1QntEjdeeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi_decoding(input_file, model_file, vocab_file, output_file, evaluate=False):\n",
        "    transition_probs, emission_probs = load_hmm_model(model_file)\n",
        "    vocabulary = load_vocabulary(vocab_file)\n",
        "\n",
        "    all_states = set(transition_probs.keys())\n",
        "\n",
        "    total_words = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file_in, open(output_file, 'w', encoding='utf-8') as file_out:\n",
        "        sentence = []\n",
        "        gold_tags = []\n",
        "\n",
        "        for line in file_in:\n",
        "            line = line.strip()\n",
        "\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    best_path = viterbi_algorithm(sentence, all_states, transition_probs, emission_probs, vocabulary)\n",
        "\n",
        "                    for i, (word, tag) in enumerate(zip(sentence, best_path), 1):\n",
        "                        file_out.write(f\"{i}\\t{word}\\t{tag}\\n\")\n",
        "\n",
        "                    if evaluate and gold_tags:\n",
        "                        for pred_tag, gold_tag in zip(best_path, gold_tags):\n",
        "                            total_words += 1\n",
        "                            if pred_tag == gold_tag:\n",
        "                                correct_predictions += 1\n",
        "\n",
        "                file_out.write(\"\\n\")\n",
        "                sentence = []\n",
        "                gold_tags = []\n",
        "                continue\n",
        "\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                word = parts[1]\n",
        "                sentence.append(word)\n",
        "\n",
        "                if evaluate and len(parts) >= 3:\n",
        "                    gold_tags.append(parts[2])\n",
        "\n",
        "    accuracy = correct_predictions / total_words if total_words > 0 else 0\n",
        "    if evaluate:\n",
        "        print(f\"Accuracy on dev data: {accuracy:.4f}\")\n",
        "\n",
        "    return accuracy if evaluate else None"
      ],
      "metadata": {
        "id": "lDQHc1JSRZm9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vocabulary(vocab_path):\n",
        "    vocabulary = set()\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if parts:\n",
        "                word = parts[0]\n",
        "                vocabulary.add(word)\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "xXpEYXM7RbYb"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function implements the Viterbi algorithm for finding the most likely sequence of POS tags for a given sentence. It works by:\n",
        "\n",
        "* Using logarithms to prevent underflow with small probabilities\n",
        "* Building a trellis of state probabilities for each word position\n",
        "* Tracking the most likely previous state with backpointers\n",
        "* Handling unknown words by substituting with an \"<unk>\" token\n",
        "* Setting minimum probability thresholds to avoid math errors\n",
        "* Finding the highest probability final state\n",
        "* Backtracking through the trellis to reconstruct the optimal tag sequence"
      ],
      "metadata": {
        "id": "Ct1KyX9ddjQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi_algorithm(words, states, transition_probs, emission_probs, vocabulary):\n",
        "    n = len(words)\n",
        "\n",
        "    LOG_ZERO = -1000.0\n",
        "\n",
        "    viterbi = [{}]\n",
        "    backpointer = [{}]\n",
        "\n",
        "    first_word = words[0]\n",
        "    if first_word not in vocabulary:\n",
        "        first_word = \"<unk>\"\n",
        "\n",
        "    for state in states:\n",
        "        emission_prob = emission_probs.get(state, {}).get(first_word, 1e-10)\n",
        "\n",
        "        emission_prob = max(emission_prob, 1e-10)\n",
        "        viterbi[0][state] = math.log(emission_prob)\n",
        "        backpointer[0][state] = None\n",
        "\n",
        "    for t in range(1, n):\n",
        "        viterbi.append({})\n",
        "        backpointer.append({})\n",
        "\n",
        "        word = words[t]\n",
        "        if word not in vocabulary:\n",
        "            word = \"<unk>\"\n",
        "\n",
        "        for curr_state in states:\n",
        "            max_prob = float('-inf')\n",
        "            best_prev_state = None\n",
        "\n",
        "            for prev_state in states:\n",
        "                trans_prob = transition_probs.get(prev_state, {}).get(curr_state, 1e-10)\n",
        "                trans_prob = max(trans_prob, 1e-10)\n",
        "\n",
        "                prob = viterbi[t-1][prev_state] + math.log(trans_prob)\n",
        "\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_prev_state = prev_state\n",
        "\n",
        "            emission_prob = emission_probs.get(curr_state, {}).get(word, 1e-10)\n",
        "            emission_prob = max(emission_prob, 1e-10)\n",
        "\n",
        "            viterbi[t][curr_state] = max_prob + math.log(emission_prob)\n",
        "            backpointer[t][curr_state] = best_prev_state\n",
        "\n",
        "    best_final_state = None\n",
        "    max_final_prob = float('-inf')\n",
        "\n",
        "    for state in states:\n",
        "        if viterbi[-1][state] > max_final_prob:\n",
        "            max_final_prob = viterbi[-1][state]\n",
        "            best_final_state = state\n",
        "\n",
        "    if best_final_state is None and states:\n",
        "        best_final_state = next(iter(states))\n",
        "\n",
        "    best_path = [best_final_state]\n",
        "    for t in range(n-1, 0, -1):\n",
        "        prev_state = backpointer[t][best_path[0]]\n",
        "        if prev_state is None and states:\n",
        "            prev_state = next(iter(states))\n",
        "        best_path.insert(0, prev_state)\n",
        "\n",
        "    return best_path"
      ],
      "metadata": {
        "id": "9ej0pw1LRtcI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = viterbi_decoding(\"dev\", \"hmm.json\", \"vocab.txt\", \"dev_viterbi.out\", evaluate=True)\n",
        "print(f\"Viterbi decoding accuracy on dev data: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF_WdFLOO6bF",
        "outputId": "41a50db4-94d1-49c5-e62a-cdc69d6a5935"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on dev data: 0.9385\n",
            "Viterbi decoding accuracy on dev data: 0.9385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "viterbi_decoding(\"test\", \"hmm.json\", \"vocab.txt\", \"viterbi.out\")\n",
        "print(f\"Viterbi decoding completed. Predictions saved in '{test_output}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xSEO7rkQIQh",
        "outputId": "a4f27b03-f6ff-40f2-c820-cb7f3451cbc7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Viterbi decoding completed. Predictions saved in 'viterbi.out'\n"
          ]
        }
      ]
    }
  ]
}